# LLM_from_scratch

## Overview

This project involves creating a simplified version of a large language model, complete with:

- **Token embeddings**

- **Positional embeddings**

- **Multi-head self-attention**

- **Feedforward layers**

- **Layer normalization**

By the end of this guide, you will have:

✅ A working LLM architecture built from scratch using PyTorch.

✅ A fully functional training loop with validation checks.

✅ Insights into training and validating a transformer-based model on tokenized text.

Read complete article on:
[Medium Blog](https://medium.com/@safniusman/build-your-own-gpt-like-model-a-beginners-guide-to-training-transformers-from-scratch-413cb2f17831)
